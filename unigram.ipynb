{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Model \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk import PorterStemmer  \n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from nltk.probability import LidstoneProbDist, WittenBellProbDist\n",
    "import sys\n",
    "from IPython.display import display\n",
    "#sys.stdout = stdout\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "stopWord = set(sw.words(\"english\"));\n",
    "import numpy as np\n",
    "import pandas as pds\n",
    "import math\n",
    "import os\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"ray_913742943690653739_cf.txt\", \"ray_925987435331224445_cf.txt\", \n",
    "        \"ray_925987537478928034_pf.txt\"]\n",
    "lam = 0.3\n",
    "topNRDoc = 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Functions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to clean document \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting word from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = ['.', ',', '!', '', \"+\", \"#\", \"(\", \")\", \":\", \"'s\"]\n",
    "slangs = {\"n't\":\"not\", \"r\": \"are\", \"u\": \"you\"}\n",
    "def goclean(doc):\n",
    "    doc = doc.lower()\n",
    "    #\\W+\n",
    "    tokens = wt(doc)\n",
    "    filterWord = [];\n",
    "    \n",
    "    for w in tokens:\n",
    "        if w not in dots and w not in stopWord:\n",
    "            if w in slangs:\n",
    "                w = slangs[w];\n",
    "            filterWord.append(w)\n",
    "    sents = \" \".join(filterWord)\n",
    "    filterWord = re.findall('\\w+', sents)\n",
    "    ps = PorterStemmer()\n",
    "    stemed = []\n",
    "    for w in filterWord:\n",
    "        fword = ps.stem(w)\n",
    "        stemed.append(fword) \n",
    "    return stemed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to calculate probability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getprobability(cleanW):\n",
    "    myarray = np.asarray(cleanW)\n",
    "    ue, ce = np.unique(myarray, return_counts=True)\n",
    "    tc = len(cleanW);\n",
    "    n = float(tc)\n",
    "    \n",
    "    p = [];\n",
    "    for i in range(len(ce)): \n",
    "        p.append(ce[i]/n)\n",
    "    \n",
    "    mydict = dict(zip(ue, p))\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing function not in use now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(probd, probc, lam):\n",
    "    dict2 = dict(probc)\n",
    "    \n",
    "    for key, items in probc.iteritems():\n",
    "        if key in probd:\n",
    "            val = (1 - lam)*probd[key] + lam*probc[key];\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "        \n",
    "        else:\n",
    "            val = lam*(probc[key]);\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "    return dict2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change in formula instead of smoothing we get prob ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpratio(probd, probc, lam):\n",
    "    dict2 = dict(probc)\n",
    "    \n",
    "    for key, items in probc.iteritems():\n",
    "        if key in probd:\n",
    "            val = ((1 - lam)*probd[key])/(lam*probc[key]) + 1;\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "        \n",
    "        else:\n",
    "            val = 1\n",
    "            #pdf[name][key] = val;\n",
    "            dict2[key] = val\n",
    "    return dict2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inverse indexing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping words to document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordMap(clrev, wordMap, i):\n",
    "    for key in clrev:\n",
    "        wordMap[key].append(i)\n",
    "    return wordMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging wordMap for a query to get the list of doc which contain the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMerge(wordMap, word):\n",
    "    docList = []\n",
    "    for w in word:\n",
    "        if w not in pdf.index:\n",
    "            continue;\n",
    "        docList= list(set(docList + wordMap[w]))\n",
    "    return docList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence / Ranking function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdivergence(q, df, docList):\n",
    "    TD = len(docList)\n",
    "    x = [0]*TD;\n",
    "    for word, prob in q.iteritems():\n",
    "        if word not in pdf.index:\n",
    "            continue;\n",
    "        for i in range(TD):\n",
    "            name = 'probd' + str(docList[i])\n",
    "            x[i] = x[i] + prob*(math.log(pdf[name][word]))\n",
    "    return x;       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working on collection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\n",
    "def getdoc(files, doc):\n",
    "    for f in files:\n",
    "        doc += open(f , 'r').read()\n",
    "    return doc\n",
    "\n",
    "doc += getdoc(files, doc);\n",
    "doc = doc.decode('utf-8');\n",
    "cw = goclean(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "probc = getprobability(cw)\n",
    "dict2 = dict(probc)\n",
    "pdf = pds.DataFrame(dict2.values(), index=dict2.keys(), columns=[\"probc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = len(probc)\n",
    "values = [[] for _ in range(tc)]\n",
    "keys = dict2.keys()\n",
    "wordMap = dict(zip(keys, values)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on documents / Doc model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = doc.split(\"\\n\")\n",
    "td = len(reviews)\n",
    "for i in range(len(reviews)):\n",
    "    name = \"probd\" + str(i);\n",
    "    \n",
    "    clrev = goclean(reviews[i]) # ['what', 'should', 'not', 'be', 'done'] without removing stopword\n",
    "    \n",
    "    wordMap = getWordMap(clrev, wordMap, i) #{u'andra': [1084, 1084], u'authorit': [254, 255, 784, 254, 255, 784],..}\n",
    "    \n",
    "    prob = getprobability(clrev) #{'be': 0.20000000000000001,'done': 0.20000000000000001,........}\n",
    "    \n",
    "    ps = getpratio(prob, probc, lam) #((1 - lam)*probd[key])/(lam*probc[key]) + 1;\n",
    "    \n",
    "    pdf[name] = ps.values() #including the doc model in data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query modeling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Netflix\n"
     ]
    }
   ],
   "source": [
    "#q = (reviews[16] + '.')[:-1]\n",
    "q = raw_input() # \"what shouldn't be done\"\n",
    "\n",
    "cq = goclean(q) # ['what', 'should', 'not', 'be', 'done'] without removing stopword\n",
    "\n",
    "qprob = getprobability(cq) #{'be': 0.20000000000000001,'done': 0.20000000000000001,........}  \n",
    "\n",
    "docList = getMerge(wordMap, cq) #[1195, 4, 519, 522, 524, .......] list containg words in query\n",
    "\n",
    "div = KLdivergence(qprob, pdf, docList) #ranking function using KL divergens\n",
    "\n",
    "index = div.index(max(div)) #most probabel relevent doc\n",
    "\n",
    "#reviews[docList[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top N Relevent doc\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1) I really liked how you related technical references back to your daily life and how you use amazon services and that of our customers, Alexa, Netflix.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'2)  great overview of ML experience at Amazon and other customers (Zillow, pinterest, netflix, well structured to follow you through the three-tier AI/ML model like the separation of framework in different categories and examples'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'3)  great overview of ML experience at Amazon love the statement: we want to share the experience from other customers and ourselves with you good highlighting freedom of choice good customer examples (netflix) nice wrap-up (history, 3-tier model, breadth & depth of platform)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'4) deep use of amazon use cases to showcase our AI/ML capabilities; strong use of references (e.g., Netflix, AHA, etc.); very complete use of the use cases; deep review of each level of AI/ML; nice tie backs to our infrastructure (P2, etc.);'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'5) Strong intro with summary of internal Amazon use cases over time. Like how you presented the choice we offer and support for major frameworks. Good job with overview of the 3 layers, and like how you weaved in customer reference examples like Netflix. Overall solid job establishing our leadership.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u\"6) Really good job learning and working in the customer cases (Zillow, Netflix, CSPAN, American Heart Association). Nailed the core message of three main points of 1) Amazon ML heritage, 2) the three tier AWS ML platform and 3) the three main benefits to ML from the overall AWS cloud platform of a) S3 for scalable storage tier b) high security and c) the most robust analytics offering. Also great job calling out AWS' differentiation around taking an open approach to ML/DL frameworks that other vendors are not doing.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'7) Great use of customer examples Netflix, Expedia and Bandlab Very calm and measured delivery'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'8) Nice job weaving in Zillow and Netflix. Great job including analytics and data lake concepts'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u'9) Super smooth delivery. Great understanding of the stack layers. Loved how you described NetFlix use case.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "u\"10) Nailed the three core components of the message of 1) Amazon's strong 20 year heritage in AI-ML 2) the AWS three tier ML paltform and 3) the three main benefits of the overall AWS cloud for ML of a) S3 as a scalable and highly cost effective data lake b) AWS is a highly secure environment and c) we have the broadest analytics tools portfolio beyond ML tools. Great detail on each of the three layers of our ML platform and especially on our open support of the various ML/DL frameworks which is a key AWS differentiator. Awesome calling out the Zillow, Netflix and Liberty Mutual customer examples.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tlist = div[:];\n",
    "x = []#*topNRDoc\n",
    "\n",
    "for i in range(min(topNRDoc, len(tlist))):\n",
    "    Pmax = max(tlist)\n",
    "    #if least == 0:\n",
    "        #break\n",
    "    index = tlist.index(Pmax);\n",
    "    x.append(docList[index])\n",
    "    tlist[index] = -1;\n",
    "\n",
    "    \n",
    "rdl = len(x)\n",
    "if rdl == 0:\n",
    "    print(\"No relevent document found\")\n",
    "    \n",
    "else:\n",
    "    prt = \"\"\n",
    "    for i in range(rdl):\n",
    "        display(str(i+1) + ') ' + reviews[x[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
